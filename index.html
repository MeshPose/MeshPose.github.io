<!DOCTYPE html>

<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="DensePose,Mesh,Reconstruction,Reprojection">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>MeshPose Project page</title>
  <link rel="icon" type="image/x-icon" href="static/images/logo_snapchat.jpeg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MeshPose: Unifying DensePose and 3D Body Mesh reconstruction</h1>
            <div class="is-size-5 publication-authors">

              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://erictuanle.com/" target="_blank">Eric-Tuan Le</a><sup>*</sup><sup>1</sup>,</span>
              <span class="author-block">
                <a>Antonis Kakolyris</a><sup>*</sup><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://robotics.ntua.gr/members/pkoutras/" target="_blank">Petros Koutras</a><sup>2</sup>,</span>
              <span class="author-block">
                <a>Himmy Tam</a><sup>2</sup>,</span>
              <span class="author-block">
                <a>Efstratios Skordos</a><sup>2</sup>,</span>
              <span class="author-block">
                <a>George Papandreou</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="http://alpguler.com/" target="_blank">Riza Alp Guler</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="http://www0.cs.ucl.ac.uk/staff/i.kokkinos/" target="_blank">Iasonas Kokkinos</a><sup>2</sup></span>
            </div>

            <!-- Paper affiliation -->
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>UCL&emsp;&emsp;&emsp;&emsp;<sup>2</sup>Snap Inc.<br>CVPR 2024</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Equal contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- Paper -->
                <span class="link-block">
                  <a href="static/pdf/meshpose_paper.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Poster -->
                <span class="link-block">
                  <a href="static/pdf/meshpose_poster.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Poster</span>
                  </a>
                </span>


              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Video -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <center>
          <video loop controls muted autoplay playsinline class="video">
            <source src="https://github.com/MeshPose/MeshPose.github.io/raw/main/static/videos/meshpose_video.m4v" type="video/mp4">
          </video>
          </center>
        </div>
      </div>
    </div>
  </section>

  <!-- Abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              DensePose provides a pixel-accurate association of images with 3D mesh coordinates, but does not provide a 3D mesh, while
              Human Mesh Reconstruction (HMR) systems have high 2D reprojection error, as measured by DensePose localization metrics.
              In this work we introduce MeshPose to jointly tackle DensePose and HMR. For this we first introduce new losses that allow us
              to use weak DensePose supervision to accurately localize in 2D a subset of the mesh vertices ('VertexPose'). We then lift
              these vertices to 3D, yielding a low-poly body mesh ('MeshPose'). Our system is trained in an end-to-end manner and is the
              first HMR method to attain competitive DensePose accuracy, while also being lightweight and amenable to efficient inference,
              making it suitable for real-time AR applications.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Pipeline -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered"><h2 class="title is-3">MeshPose Architecture</h2></div>
      <div class="container">
        <img src="static/images/pipeline.png" alt="Meshpose Architecture">
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="content has-text-justified">
          <p style="margin-bottom:1cm;">
            The lower VertexPose branch extracts multiple heatmaps from which, by applying the spatial argsoftmax
            operation, it computes precise x and y coordinates for all the vertices inside the input crop. The upper Regression branch
            computes the coordinates (x, y, and vertex depth z) for all vertices, along with their visibility scores w. The score w will
            take lower values when the corresponding vertex is either occluded or fall outside the crop area. We differentiably combine the
            VertexPose and regressed coordinates via w to get the final 3D mesh. We densely supervise the intermediate per-vertex heatmaps
            and the final output with UV, mesh and silhouette cues to end up with a low latency, image aligned, in-the-wild HMR system
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Vertex Pose -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered"><h2 class="title is-3">VertexPose Supervision</h2></div>
      <div class="container">
        <center> <img src="static/images/vertexpose_supervision.png" alt="VertexPose Supervision"> </center>
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="content has-text-justified">
          <p style="margin-bottom:1cm;">
            Geometry-driven losses used to supervise VertexPose with DensePose ground-truth. Our barycentric loss requires that the
            per-pixel distribution over VertexPose matches the UV annotation’s barycentrics. Our UV consistency loss requires that the
            UV annotation’s barycentrics at a labelled pixel x should recover x based on a similar combination of VertexPose vertices into x&#770;.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Mesh Pose -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered"><h2 class="title is-3">MeshPose Supervision</h2></div>
      <center>
        <div class="container">
          <img src="static/images/meshpose_supervision1.png" width=80% alt="Meshpose Supervision">
        </div>
    </center>
    </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="content has-text-justified">
          <p>
            <b>Visibility Supervision</b>: We estimate partial vertex visibility based on the available ground-truth: for any (x, u)
            annotation pair contained in the DensePose dataset, we declare as visible all three vertices that lie on the mesh triangle
            containing u. We also declare as non-visible every vertex where the mesh supervision is outside the image crop. For
            such vertices we can supervise visibility based on a standard binary cross-entropy loss.
          </p>
          <p style="margin-bottom:1cm;">
            <b>Branch Fusion</b>: The 2D location of a MeshPose vertex is the visibility-weighted average between VertexPose-based 2D
            positions and regressed values. The visibility label dictates on a per-vertex level whether we should rely on the
            VertexPose-based 2D position or fall back to the value regressed at this stage. This allows us to accommodate occluded areas,
            or tight crops that omit part of the human body, as is regularly the case for selfie images. This differentiable expression allows us to estimate visibility
            through end-to-end back-propagation, but we also use two additional methods for visibility supervision.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Quantitative Results -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered"><h2 class="title is-3">Quantitative Results</h2></div>
      
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="content has-text-justified">
            <p style="margin-bottom:1cm;margin-top:0.5cm">
              We outperform HMR methods on DensePose metrics by more than 50% while having close to state of the art 3D accuracy. By
              combining the highest FPS rate and small model size with state-of-art reprojection accuracy, our pipeline is well suited
              for mobile inference.
            </p>
          </div>
        </div>
      </div>
      <div class="container">
        <div class="columns is-centered has-text-centered"><img width="80%" src="static/images/quantitative_diagram.png" alt="Quantitative Evaluation"></div>
      </div>
    </div>

    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="content has-text-justified">
            <p style="margin-bottom:1cm;">
              Our models are purely convolutional and thus run out-of-the-box on modern phones with accelerators. We exported the ONNX
              versions of our models and computed their timings (FPS) on an iPhone-12 using the CoreML backend, obtaining comparable timings
              to GPU-desktop timings.
            </p>
          </div>
        </div>
      </div>
      <div class="container">
        <div class="columns is-centered has-text-centered"><img width="80%" src="static/images/mobile_fps.png" alt="Mobile inference"></div>
      </div>

    </div>
  </div>
  </section>

  <!-- 3DPW Videos -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered"><h2 class="title is-3">3DPW Videos</h2></div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
            <table width="100%">
              <tr>
                <td width="48%">
                  <video muted autoplay loop>
                    <source src="https://github.com/MeshPose/MeshPose.github.io/raw/main/static/videos/3dpw_1.mp4" width="48%" type="video/mp4">
                  </video>
                </td>
                <td width="48%">
                  <video muted autoplay loop>
                    <source src="https://github.com/MeshPose/MeshPose.github.io/raw/main/static/videos/3dpw_3.mp4" width="48%" type="video/mp4">
                  </video>
                </td>
              </tr>
            </table>
        </div>
      </div>

    </div>
  </section>

  <!-- In the Wild Videos -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered"><h2 class="title is-3">In-the-Wild Videos</h2></div>
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="content has-text-justified">
            <p style="margin-bottom:1cm;">
              We demonstrate very strong temporal stability (low jitter) even when applied frame-by-frame without any post-processing.
              In the videos below, only the detected bounding box is temporally-smoothed.
            </p>
          </div>
        </div>
      </div>

      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <center>
            <table>
              <tr>
                <td align="center" valign="top" width="48%">
                  <video muted autoplay loop>
                    <source src="https://github.com/MeshPose/MeshPose.github.io/raw/main/static/videos/video1.mp4" width="48%" type="video/mp4">
                  </video>
                </td>
                <td align="center" valign="top" width="48%">
                  <video muted autoplay loop>
                    <source src="https://github.com/MeshPose/MeshPose.github.io/raw/main/static/videos/video2.mp4" width="48%" type="video/mp4">
                  </video>
                </td
              </tr>
            </table>
            <table>
              <tr>
                <td align="center" valign="top" width="48%">
                  <video muted autoplay loop>
                    <source src="https://github.com/MeshPose/MeshPose.github.io/raw/main/static/videos/video3.mp4" width="48%" type="video/mp4">
                  </video>
                </td>
                <td align="center" valign="top" width="48%">
                  <video muted autoplay loop>
                    <source src="https://github.com/MeshPose/MeshPose.github.io/raw/main/static/videos/video4.mp4" width="48%" type="video/mp4">
                  </video>
                </td
              </tr>
            </table>
          </center>
        </div>
      </div>
    </div>
  </section>

  <!-- Bibtex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{lekakolyris2024meshpose,
          title={MeshPose: Unifying DensePose and 3D Body Mesh reconstruction},
          author={Eric-Tuan Le\^, Antonis Kakolyris, Petros Koutras, Himmy Tam, Efstratios Skordos, George Papandreou, R\{i}za Alp G\"uler, Iasonas Kokkinos},
          booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
          year={2024}
        }        
      </code></pre>
    </div>
  </section>

  <!-- Credit -->
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->
  
  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>
</html>
